{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIFf6_DEoyIK",
        "outputId": "93a0cd24-f771-4649-c313-08f0b4ab0356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=b7bccdb3e42bd9a019b382f19a11828db926bc84126124f24b772a47fa30b4cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install sacremoses\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUDD5vSTP4B_"
      },
      "source": [
        "# server\n",
        "\n",
        "\n",
        "```\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pFgfP5Yjo2OG"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, MarianMTModel\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "import re\n",
        "\n",
        "\n",
        "device=0 if torch.cuda.is_available() else -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcywEekIX1ji",
        "outputId": "0b8cc9c4-0717-41f2-87cf-07753d93c4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.7/698.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.8/529.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.1/439.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.1/439.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fiona 1.9.4.post1 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.4.5 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install -q colabcode\n",
        "!pip install -q fastapi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "geyScMaGaRnD"
      },
      "outputs": [],
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI\n",
        "\n",
        "import numpy as np\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HlXex-RWadl9"
      },
      "outputs": [],
      "source": [
        "cc = ColabCode(port=12000, code=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MQn0yL5ec3R-"
      },
      "outputs": [],
      "source": [
        "# %%writefile models.py\n",
        "from pydantic import BaseModel, conlist\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class X(BaseModel):\n",
        "    data: List[conlist(str, min_items=2, max_items=2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3Y0JkGhfajw7"
      },
      "outputs": [],
      "source": [
        "#waaaaaaat tengo que definir los modelos en la misma celda????\n",
        "# def model(strings_list):\n",
        "#   return strings_list[0]+strings_list[1]\n",
        "context='''I would like a salary around 800000 pesos, i have about 2 years of experience in programing in general,expect in python i have 5, and have 2 years of experience in the rest of things.\n",
        "\n",
        "mi curriculum is:\n",
        "\n",
        "AGUSTÍN BUSTOS BARTON\n",
        "agustinbustosbarton@gmail.com | +54-9-11-6514-0295 | Morón, Bs As, Argentina |\n",
        "Portfolio: https://agustinbustos.github.io\n",
        "Personal Project: https://www.innorma.eu\n",
        "LinkedIn: https://www.linkedin.com/in/agustinbustos/\n",
        "Tech\n",
        "Python (Advanced level in skLearn and Pytorch , Pandas, Numpy, Polars, Selenium).\n",
        "SQL\n",
        "Azure Cloud Computing for Big Data Analysis.\n",
        "Hugging Face NLP modeling.\n",
        "------------------->>> Github: https://github.com/AgustinBustos\n",
        "Experience\n",
        "2022-Ongoing: Market Fusion Analytics, Data Analyst:\n",
        "Creation of Python Data analysis tools (SMjour library) for statistical modeling with focus on the\n",
        "following topics:\n",
        "• column selection for control variables (macro & covid variables)\n",
        "• collinearity problem (media spending and ROI estimation)\n",
        "• non-linear transformation analysis (decreasing returns on investment)\n",
        "Education\n",
        "2021: Bachelor of Economics, Universidad Torcuato Di Tella.\n",
        "• Minor field in Mathematics and Statistics.\n",
        "• Scholarship ‘Mejores Promedios de Colegio Público’ from Universidad Torcuato Di\n",
        "Tella.\n",
        "2016: Basic Musical Formation, Conservatorio de Música de Morón “Alberto Ginastera”.\n",
        "Languages\n",
        "Spanish (native), English (professional).\n",
        "Personal Description\n",
        "Casio fx-82ms was illegal in elementary school, you had to do the calculations mentally, but once our\n",
        "knowledge is advanced enough, in high school, it could already be used; at Uni, the fx-991 LA was borderline\n",
        "legal, but using the TI-Nspire CX II or GeoGebra was not possible; once university is finished, in economics,\n",
        "calculators such as R, Python, Excel, Stata are used, but what is the next step? What are the best calculators\n",
        "of humanity? The time for GPUs is upon the econometric models.\n",
        "also love coffee!! :) '''\n",
        "\n",
        "####################3es en model\n",
        "src = \"es\"  # source language\n",
        "trg = \"en\"  # target language\n",
        "\n",
        "model_name2 = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
        "model_esen = MarianMTModel.from_pretrained(model_name2)\n",
        "tokenizer_esen = AutoTokenizer.from_pretrained(model_name2)\n",
        "###################question model\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "###################### en es model\n",
        "src = \"en\"  # source language\n",
        "trg = \"es\"  # target language\n",
        "\n",
        "model_name3 = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
        "model_enes = MarianMTModel.from_pretrained(model_name3)\n",
        "tokenizer_enes = AutoTokenizer.from_pretrained(model_name3)\n",
        "\n",
        "\n",
        "def full_model(strings_list):\n",
        "  pregunta=strings_list[1]+', '+strings_list[0]\n",
        "\n",
        "  #translate\n",
        "  batch = tokenizer_esen([pregunta], return_tensors=\"pt\")\n",
        "  generated_ids = model_esen.generate(**batch)\n",
        "  question=tokenizer_esen.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  # solve question\n",
        "  QA_input = {\n",
        "    'question': question,\n",
        "    'context': context\n",
        "  }\n",
        "  answer = nlp(QA_input)\n",
        "  #translate back\n",
        "  batch = tokenizer_enes([answer['answer']], return_tensors=\"pt\")\n",
        "  generated_ids = model_enes.generate(**batch)\n",
        "  final_answer=tokenizer_enes.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "  return re.findall(r'\\d+',final_answer )[0] if len(re.findall(r'\\d+',final_answer ))>0 else final_answer #final_answer\n",
        "full_model('how much is your expected salary?')\n",
        "\n",
        "\n",
        "\n",
        "app = FastAPI(title=\"ML\", description=\"with FastAPI and ColabCode\", version=\"1.0\")\n",
        "\n",
        "\n",
        "# model = None\n",
        "# @app.on_event(\"startup\")\n",
        "# def load_model():\n",
        "#     global model\n",
        "#     model = pickle.load(open(\"model_gb.pkl\", \"rb\"))\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"go to\": \"/docs\"}\n",
        "\n",
        "\n",
        "@app.post(\"/api\", tags=[\"prediction\"])\n",
        "async def get_predictions(x: X):\n",
        "\n",
        "    try:\n",
        "      global model\n",
        "\n",
        "      data = dict(x)['data'][0]   #[0]\n",
        "\n",
        "      # print('problem with model')\n",
        "      prediction = full_model(data[0])\n",
        "      print(prediction)\n",
        "      return {\"prediction\": prediction}\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return {\"prediction\": \"error\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSosEJcaukR",
        "outputId": "aa99bc40-505c-4102-db69-dd4062fc7edf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-09-14T17:56:32+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://1cef-35-230-104-156.ngrok-free.app\" -> \"http://localhost:12000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [525]\n",
            "INFO:uvicorn.error:Started server process [525]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "5\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "800000\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2021\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2021\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2021\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "54\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "800000\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Licenciado en Economía\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Licenciado en Economía\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Español\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Licenciado en Economía\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "800000\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2021\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "5\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Licenciado en Economía\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Licenciado en Economía\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2022\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "Licenciado en Economía\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "2\n",
            "INFO:     190.190.99.71:0 - \"POST /api HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:uvicorn.error:Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:uvicorn.error:Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:uvicorn.error:Application shutdown complete.\n",
            "INFO:     Finished server process [525]\n",
            "INFO:uvicorn.error:Finished server process [525]\n"
          ]
        }
      ],
      "source": [
        "cc.run_app(app=app)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
